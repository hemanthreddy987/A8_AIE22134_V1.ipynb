{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A3 \n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
    "    # Count the occurrences of each class\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    # Filter out zero probabilities to avoid math error in np.log2\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def conditional_entropy(x, y):\n",
    "    \"\"\"Calculate the conditional entropy of y given x.\"\"\"\n",
    "    # Entropy accumulator\n",
    "    entropy_acc = 0\n",
    "    # Iterate over each category in x to calculate conditional entropy\n",
    "    for value in np.unique(x):\n",
    "        subset_y = y[x == value]\n",
    "        entropy_acc += (len(subset_y) / len(y)) * entropy(subset_y)\n",
    "    return entropy_acc\n",
    "\n",
    "def information_gain(X, y, feature_index):\n",
    "    \"\"\"Calculate the Information Gain of a feature.\"\"\"\n",
    "    return entropy(y) - conditional_entropy(X[:, feature_index], y)\n",
    "\n",
    "def find_root_node(X, y):\n",
    "    \"\"\"Find the feature index that should be used as the root node.\"\"\"\n",
    "    gains = [information_gain(X, y, feature_index) for feature_index in range(X.shape[1])]\n",
    "    return np.argmax(gains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_continuous_values(x, n_bins=3, strategy=\"width\"):\n",
    "    \"\"\"Bin continuous values into categorical.\"\"\"\n",
    "    if strategy == \"width\":\n",
    "        # Equal width binning\n",
    "        bins = np.linspace(np.min(x), np.max(x), n_bins + 1)\n",
    "        return np.digitize(x, bins) - 1\n",
    "    elif strategy == \"frequency\":\n",
    "        # Equal frequency binning (quantiles)\n",
    "        quantiles = np.quantile(x, np.linspace(0, 1, n_bins + 1))\n",
    "        return np.digitize(x, quantiles) - 1\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported binning strategy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.predictions = None\n",
    "\n",
    "    def fit(self, X, y, n_bins=3, bin_strategy=\"width\"):\n",
    "        # Bin continuous values if necessary\n",
    "        if X.dtype == np.float:\n",
    "            for i in range(X.shape[1]):\n",
    "                X[:, i] = bin_continuous_values(X[:, i], n_bins, bin_strategy)\n",
    "        \n",
    "        # Find the root node using Information Gain\n",
    "        self.root = find_root_node(X, y)\n",
    "        \n",
    "        # Store class predictions based on the selected feature for simplicity\n",
    "        self.predictions = {}\n",
    "        for value in np.unique(X[:, self.root]):\n",
    "            subset_y = y[X[:, self.root] == value]\n",
    "            self.predictions[value] = Counter(subset_y).most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if X.dtype == np.float:\n",
    "            raise ValueError(\"Prediction data must be binned categorical values\")\n",
    "        \n",
    "        # Predict based on the majority class of the split by the root feature\n",
    "        return np.array([self.predictions.get(x[self.root], None) for x in X])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D , MaxPooling2D , Flatten , Activation , Dense , Dropout , BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam , Adamax\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_data_path = r\"C:\\Users\\HP\\Desktop\\train\"\n",
    "filepaths =[]\n",
    "labels = []\n",
    "folds = os.listdir(train_data_path)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(train_data_path , fold)\n",
    "    filelists = os.listdir(f_path)\n",
    "    \n",
    "    for file in filelists:\n",
    "        filepaths.append(os.path.join(f_path , file))\n",
    "        labels.append(fold)\n",
    "        \n",
    "Fseries = pd.Series(filepaths , name = 'filepaths')\n",
    "Lseries = pd.Series(labels , name = 'label')\n",
    "train_df = pd.concat([Fseries , Lseries] , axis = 1)\n",
    "train_df\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data_path = r\"C:\\Users\\HP\\Desktop\\train\"\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(train_data_path)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(train_data_path, fold)\n",
    "    filelists = os.listdir(f_path)\n",
    "    \n",
    "    for file in filelists:\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "images = []\n",
    "for filepath in filepaths:\n",
    "    img = cv2.imread(filepath)\n",
    "    # Resize the image to a fixed size\n",
    "    img = cv2.resize(img, (100, 100))  # Adjust the size as needed\n",
    "    img_vector = img.flatten()\n",
    "    # Append the image vector to the list\n",
    "    images.append(img_vector)\n",
    "\n",
    "# Convert the images list to a numpy array\n",
    "images_array = np.array(images)\n",
    "\n",
    "# Create DataFrame\n",
    "Fseries = pd.Series(images_array.tolist(), name='image_vector')\n",
    "Lseries = pd.Series(labels, name='label')\n",
    "train_df = pd.concat([Fseries, Lseries], axis=1)\n",
    "\n",
    "print(train_df)\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming this code was executed before\n",
    "train_data_path = r\"C:\\Users\\HP\\Desktop\\train\"\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(train_data_path)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(train_data_path, fold)\n",
    "    filelists = os.listdir(f_path)\n",
    "    \n",
    "    for file in filelists:\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "# Initialize empty lists to store image data\n",
    "images = []\n",
    "for filepath in filepaths:\n",
    "    # Read image using OpenCV\n",
    "    img = cv2.imread(filepath)\n",
    "    # Resize the image to a fixed size\n",
    "    img = cv2.resize(img, (100, 100)) \n",
    "    img_vector = img.flatten()\n",
    "    # Append the image vector to the list\n",
    "    images.append(img_vector)\n",
    "\n",
    "# Convert the images list to a numpy array\n",
    "images_array = np.array(images)\n",
    "\n",
    "# Create DataFrame\n",
    "Fseries = pd.Series(images_array.tolist(), name='image_vector')\n",
    "Lseries = pd.Series(labels, name='label')\n",
    "train_df = pd.concat([Fseries, Lseries], axis=1)\n",
    "\n",
    "# Calculate intraclass spread and interclass distances\n",
    "class_labels = train_df['label'].unique()\n",
    "\n",
    "# Initialize empty dictionaries to store mean vectors and spread vectors for each class\n",
    "class_mean_vectors = {}\n",
    "class_spread_vectors = {}\n",
    "\n",
    "# Calculate mean vector and spread vector for each class\n",
    "for label in class_labels:\n",
    "    # Filter images belonging to the current class\n",
    "    class_images = train_df[train_df['label'] == label]['image_vector']\n",
    "    \n",
    "    # Convert the image vectors to NumPy arrays\n",
    "    class_images_array = np.array([np.array(image) for image in class_images])\n",
    "    \n",
    "    # Calculate mean vector for the current class\n",
    "    mean_vector = np.mean(class_images_array, axis=0)\n",
    "    class_mean_vectors[label] = mean_vector\n",
    "    \n",
    "    # Calculate spread vector for the current class\n",
    "    spread_vector = np.std(class_images_array, axis=0)\n",
    "    class_spread_vectors[label] = spread_vector\n",
    "\n",
    "# Calculate interclass distances between mean vectors\n",
    "interclass_distances = {}\n",
    "for i in range(len(class_labels)):\n",
    "    for j in range(i + 1, len(class_labels)):\n",
    "        class1 = class_labels[i]\n",
    "        class2 = class_labels[j]\n",
    "        centroid1 = class_mean_vectors[class1]\n",
    "        centroid2 = class_mean_vectors[class2]\n",
    "        distance = np.linalg.norm(centroid1 - centroid2)\n",
    "        interclass_distances[(class1, class2)] = distance\n",
    "\n",
    "# Print intraclass spread and interclass distances\n",
    "print(\"Intraclass Spread (Standard Deviation):\")\n",
    "for label, spread_vector in class_spread_vectors.items():\n",
    "    print(f\"{label}: {spread_vector}\")\n",
    "\n",
    "print(\"\\nInterclass Distances between Mean Vectors:\")\n",
    "for classes, distance in interclass_distances.items():\n",
    "    print(f\"{classes[0]} - {classes[1]}: {distance}\")\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "train_data_path = r\"C:\\Users\\HP\\Desktop\\train\"\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(train_data_path)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(train_data_path, fold)\n",
    "    filelists = os.listdir(f_path)\n",
    "    \n",
    "    for file in filelists:\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "images = []\n",
    "for filepath in filepaths:\n",
    "    img = cv2.imread(filepath)\n",
    "    # Resize the image to a fixed size\n",
    "    img = cv2.resize(img, (100, 100))  # Adjust the size as needed\n",
    "    img_vector = img.flatten()\n",
    "    # Append the feature (for example, pixel intensity) to the list\n",
    "    feature_value = np.mean(img_vector)  # Example: using mean pixel intensity as the feature\n",
    "    images.append(feature_value)\n",
    "\n",
    "# Convert the feature values list to a numpy array\n",
    "feature_array = np.array(images)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(feature_array, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Feature')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and variance\n",
    "mean_value = np.mean(feature_array)\n",
    "variance_value = np.var(feature_array)\n",
    "\n",
    "print(f\"Mean: {mean_value}\")\n",
    "print(f\"Variance: {variance_value}\")\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "train_data_path = r\"C:\\Users\\HP\\Desktop\\train\"\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(train_data_path)\n",
    "\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(train_data_path, fold)\n",
    "    filelists = os.listdir(f_path)\n",
    "    \n",
    "    for file in filelists:\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "images = []\n",
    "for filepath in filepaths:\n",
    "    img = cv2.imread(filepath)\n",
    "    # Resize the image to a fixed size\n",
    "    img = cv2.resize(img, (100, 100))  # Adjust the size as needed\n",
    "    img_vector = img.flatten()\n",
    "    # Append the feature vector to the list\n",
    "    images.append(img_vector)\n",
    "\n",
    "# Convert the feature vectors list to a numpy array\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Divide the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "# Applying RandomSearchCV() for a single perceptron AND gate\n",
    "print(\" finding AND gate using the suitable Hyperparameters and applying RandomSearchCV()\")\n",
    "# Generate a random classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "# Define the model and hyperparameter distributions\n",
    "model = LogisticRegression(random_state=42)\n",
    "param_dist = {'C': uniform(loc=0, scale=4),\n",
    "              'penalty': ['l1', 'l2']}\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=100, cv=5, random_state=42,\n",
    "                                   n_jobs=-1, verbose=1)\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "random_search.fit(X, y)\n",
    "# Print the best hyperparameters and score\n",
    "print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
    "print(f\"Best score: {random_search.best_score_:.4f}\")\n",
    "import numpy as np\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a given target variable.\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Add a small value to avoid log(0)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(X, y, feature_idx):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a specific feature.\n",
    "    \"\"\"\n",
    "    # Calculate total entropy before split\n",
    "    total_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Calculate entropy after split based on the feature\n",
    "    unique_values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
    "    weighted_entropy = 0\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        subset_y = y[X[:, feature_idx] == value]\n",
    "        subset_entropy = calculate_entropy(subset_y)\n",
    "        weighted_entropy += (count / len(y)) * subset_entropy\n",
    "    \n",
    "    # Calculate information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def find_root_node(X, y):\n",
    "    \"\"\"\n",
    "    Find the root node attribute with the highest information gain.\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "    best_feature_idx = None\n",
    "    best_information_gain = -1\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        information_gain = calculate_information_gain(X, y, i)\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_feature_idx = i\n",
    "    \n",
    "    return best_feature_idx\n",
    "\n",
    "\n",
    "# Find the root node attribute with the highest information gain\n",
    "root_node_idx = find_root_node(X_train, y_train)\n",
    "print(\"Root node attribute index:\", root_node_idx)\n",
    "import numpy as np\n",
    "\n",
    "def bin_continuous_feature(feature, bins=None, binning_type='equal_width'):\n",
    "    \"\"\"\n",
    "    Bins a continuous-valued feature into categorical values.\n",
    "\n",
    "    Parameters:\n",
    "    - feature: numpy array, the continuous-valued feature to be binned.\n",
    "    - bins: int or list, optional, the number of bins to create or the bin edges.\n",
    "            If None, default to 10 bins.\n",
    "    - binning_type: str, optional, the type of binning to perform.\n",
    "                    Can be 'equal_width' or 'frequency'. Default is 'equal_width'.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array, the binned feature with categorical values.\n",
    "    \"\"\"\n",
    "    if binning_type == 'equal_width':\n",
    "        if bins is None:\n",
    "            bins = 10\n",
    "        return np.digitize(feature, np.linspace(feature.min(), feature.max(), bins + 1))\n",
    "\n",
    "    elif binning_type == 'frequency':\n",
    "        if bins is None:\n",
    "            bins = 10\n",
    "        return np.digitize(feature, np.histogram_bin_edges(feature, bins=bins))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid binning_type. Choose 'equal_width' or 'frequency'.\")\n",
    "\n",
    "# Example usage:\n",
    "# Generate some continuous-valued feature data\n",
    "continuous_feature = np.random.randn(100)\n",
    "\n",
    "# Bin the continuous feature using equal width binning with default number of bins\n",
    "binned_feature_equal_width_default = bin_continuous_feature(continuous_feature)\n",
    "print(\"Binned feature (Equal Width - Default Bins):\", binned_feature_equal_width_default)\n",
    "\n",
    "# Bin the continuous feature using equal width binning with 5 bins\n",
    "binned_feature_equal_width_5_bins = bin_continuous_feature(continuous_feature, bins=5)\n",
    "print(\"Binned feature (Equal Width - 5 Bins):\", binned_feature_equal_width_5_bins)\n",
    "\n",
    "# Bin the continuous feature using frequency binning with default number of bins\n",
    "binned_feature_frequency_default = bin_continuous_feature(continuous_feature, binning_type='frequency')\n",
    "print(\"Binned feature (Frequency - Default Bins):\", binned_feature_frequency_default)\n",
    "\n",
    "# Bin the continuous feature using frequency binning with 5 bins\n",
    "binned_feature_frequency_5_bins = bin_continuous_feature(continuous_feature, bins=5, binning_type='frequency')\n",
    "print(\"Binned feature (Frequency - 5 Bins):\", binned_feature_frequency_5_bins)\n",
    "import numpy as np\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature_index = feature_index  # Index of the feature to split on\n",
    "        self.threshold = threshold  # Threshold value for binary split\n",
    "        self.value = value  # Value to predict if leaf node\n",
    "        self.left = left  # Left child node\n",
    "        self.right = right  # Right child node\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a given target variable.\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Add a small value to avoid log(0)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(X, y, feature_idx):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a specific feature.\n",
    "    \"\"\"\n",
    "    # Calculate total entropy before split\n",
    "    total_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Calculate entropy after split based on the feature\n",
    "    unique_values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
    "    weighted_entropy = 0\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        subset_y = y[X[:, feature_idx] == value]\n",
    "        subset_entropy = calculate_entropy(subset_y)\n",
    "        weighted_entropy += (count / len(y)) * subset_entropy\n",
    "    \n",
    "    # Calculate information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def bin_continuous_feature(feature, bins=None, binning_type='equal_width'):\n",
    "    \"\"\"\n",
    "    Bins a continuous-valued feature into categorical values.\n",
    "\n",
    "    Parameters:\n",
    "    - feature: numpy array, the continuous-valued feature to be binned.\n",
    "    - bins: int or list, optional, the number of bins to create or the bin edges.\n",
    "            If None, default to 10 bins.\n",
    "    - binning_type: str, optional, the type of binning to perform.\n",
    "                    Can be 'equal_width' or 'frequency'. Default is 'equal_width'.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array, the binned feature with categorical values.\n",
    "    \"\"\"\n",
    "    if binning_type == 'equal_width':\n",
    "        if bins is None:\n",
    "            bins = 10\n",
    "        return np.digitize(feature, np.linspace(feature.min(), feature.max(), bins + 1))\n",
    "\n",
    "    elif binning_type == 'frequency':\n",
    "        if bins is None:\n",
    "            bins = 10\n",
    "        return np.digitize(feature, np.histogram_bin_edges(feature, bins=bins))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid binning_type. Choose 'equal_width' or 'frequency'.\")\n",
    "\n",
    "def find_root_node(X, y, binning_type='equal_width', bins=None):\n",
    "    \"\"\"\n",
    "    Find the root node attribute with the highest information gain.\n",
    "    Parameters:\n",
    "    - X: numpy array, the feature matrix.\n",
    "    - y: numpy array, the target variable.\n",
    "    - binning_type: str, optional, the type of binning to perform.\n",
    "                    Can be 'equal_width' or 'frequency'. Default is 'equal_width'.\n",
    "    - bins: int or list, optional, the number of bins to create or the bin edges.\n",
    "    Returns:\n",
    "    - int, the index of the feature to be used as the root node attribute.\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "    best_feature_idx = None\n",
    "    best_information_gain = -1\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        if len(np.unique(X[:, i])) == 1:  # Skip features with only one unique value\n",
    "            continue\n",
    "\n",
    "        if np.issubdtype(X[:, i].dtype, np.floating):  # Continuous-valued feature\n",
    "            binned_feature = bin_continuous_feature(X[:, i], bins=bins, binning_type=binning_type)\n",
    "            information_gain = calculate_information_gain(binned_feature, y, i)\n",
    "        else:  # Categorical feature\n",
    "            information_gain = calculate_information_gain(X, y, i)\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_feature_idx = i\n",
    "    \n",
    "    return best_feature_idx\n",
    "\n",
    "def build_decision_tree(X, y, binning_type='equal_width', bins=None):\n",
    "    \"\"\"\n",
    "    Build a decision tree recursively.\n",
    "    Parameters:\n",
    "    - X: numpy array, the feature matrix.\n",
    "    - y: numpy array, the target variable.\n",
    "    - binning_type: str, optional, the type of binning to perform.\n",
    "                    Can be 'equal_width' or 'frequency'. Default is 'equal_width'.\n",
    "    - bins: int or list, optional, the number of bins to create or the bin edges.\n",
    "    Returns:\n",
    "    - TreeNode, the root node of the decision tree.\n",
    "    \"\"\"\n",
    "    if len(np.unique(y)) == 1:  # If all samples in node have the same class, return leaf node\n",
    "        return TreeNode(value=y[0])\n",
    "\n",
    "    root_feature_idx = find_root_node(X, y, binning_type=binning_type, bins=bins)\n",
    "\n",
    "    if root_feature_idx is None:  # If no feature provides information gain, return leaf node\n",
    "        return TreeNode(value=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "    if np.issubdtype(X[:, root_feature_idx].dtype, np.floating):  # Continuous-valued feature\n",
    "        binned_feature = bin_continuous_feature(X[:, root_feature_idx], bins=bins, binning_type=binning_type)\n",
    "        threshold = np.median(binned_feature)\n",
    "        left_indices = np.where(binned_feature <= threshold)[0]\n",
    "        right_indices = np.where(binned_feature > threshold)[0]\n",
    "        left_child = build_decision_tree(X[left_indices], y[left_indices], binning_type=binning_type, bins=bins)\n",
    "        right_child = build_decision_tree(X[right_indices], y[right_indices], binning_type=binning_type, bins=bins)\n",
    "        return TreeNode(feature_index=root_feature_idx, threshold=threshold, left=left_child, right=right_child)\n",
    "\n",
    "    else:  # Categorical feature\n",
    "        unique_values = np.unique(X[:, root_feature_idx])\n",
    "        children = {}\n",
    "        for value in unique_values:\n",
    "            indices = np.where(X[:, root_feature_idx] == value)[0]\n",
    "            children[value] = build_decision_tree(X[indices], y[indices], binning_type=binning_type, bins=bins)\n",
    "        return TreeNode(feature_index=root_feature_idx, value=children)\n",
    "\n",
    "# Example usage with the provided dataset\n",
    "# Assuming X_train is your feature matrix and y_train is your target variable\n",
    "# Replace this with your actual dataset\n",
    "\n",
    "# Build the decision tree\n",
    "decision_tree = build_decision_tree(X_train, y_train)\n",
    "\n",
    "# Example usage: Predict using the decision tree\n",
    "def predict(tree, sample):\n",
    "    if tree.value is not None:  # If leaf node, return value\n",
    "        return tree.value\n",
    "    elif np.issubdtype(sample[tree.feature_index].dtype, np.floating):  # Continuous-valued feature\n",
    "        if sample[tree.feature_index] <= tree.threshold:\n",
    "            return predict(tree.left, sample)\n",
    "        else:\n",
    "            return predict(tree.right, sample)\n",
    "    else:  # Categorical feature\n",
    "        child = tree.value.get(sample[tree.feature_index])\n",
    "        if child is None:\n",
    "            return None  # Missing value, return None\n",
    "        else:\n",
    "            return predict(child, sample)\n",
    "\n",
    "# Example usage: Predict using the decision tree\n",
    "sample = np.array(images)\n",
    "print(\"Prediction:\", predict(decision_tree, sample))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
